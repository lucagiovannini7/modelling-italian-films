{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Italian Cinema Data Mining & Geocoding\n",
        "\n",
        "| Section | Description | Output |\n",
        "|---------|-------------|--------|\n",
        "| **1. Setup** | Install dependencies, load libraries | - |\n",
        "| **2. Scraping** | Extract film data from website | `filmitalia_raw.csv` |\n",
        "| **3. Cleanup** | Clean and enrich data | `filmitalia_cleaned.csv` |\n",
        "| **4. Location Extraction** | Extract from **synopsis** AND **wikipedia_summary** | `filmitalia_locations.csv` |\n",
        "| **5. Geocoding** | Geocode ALL locations via Nominatim | `filmitalia_geocoded.csv` |\n",
        "| **6. Visualization** | Maps and comparative analysis | `database.csv` |\n",
        "\n",
        "**Output columns:** `locations_synopsis`, `coordinates_synopsis`, `locations_wiki`, `coordinates_wiki`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. SETUP & IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load('it_core_news_sm')\n",
        "except:\n",
        "    !python -m spacy download it_core_news_sm\n",
        "    nlp = spacy.load('it_core_news_sm')\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. WEB SCRAPING\n",
        "\n",
        "**⚠️ Set `RUN_SCRAPING = True` to execute**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_SCRAPING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_SCRAPING:\n",
        "    \n",
        "    def scrape_film_urls(page_num, retries=3, delay=2):\n",
        "        url = f\"https://filmitalia.org/it/film/pag-{page_num}/\"\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                film_links = []\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    href = link['href']\n",
        "                    if '/it/film/' in href and href.endswith('/'):\n",
        "                        if href.startswith('/'):\n",
        "                            href = 'https://filmitalia.org' + href\n",
        "                        film_links.append(href)\n",
        "                return list(set(film_links))\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for page {page_num}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(delay)\n",
        "        return []\n",
        "\n",
        "    print(\"Scraping film URLs...\")\n",
        "    all_film_urls = []\n",
        "    for page in tqdm(range(1, 500)):\n",
        "        urls = scrape_film_urls(page)\n",
        "        if not urls:\n",
        "            print(f\"No URLs found on page {page}. Stopping.\")\n",
        "            break\n",
        "        all_film_urls.extend(urls)\n",
        "        time.sleep(1)\n",
        "\n",
        "    df_urls = pd.DataFrame({'film_url': list(set(all_film_urls))})\n",
        "    df_urls.to_csv('filmitalia_urls.csv', index=False)\n",
        "    print(f\"Scraped {len(df_urls)} unique film URLs\")\n",
        "\n",
        "    def scrape_film_details(film_url, retries=3, delay=2):\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.get(film_url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                film_data = {'film_url': film_url}\n",
        "                title_elem = soup.find('h1')\n",
        "                film_data['title'] = title_elem.text.strip() if title_elem else None\n",
        "                for row in soup.find_all('div', class_='field'):\n",
        "                    label = row.find('div', class_='field-label')\n",
        "                    value = row.find('div', class_='field-items')\n",
        "                    if label and value:\n",
        "                        field_name = label.text.strip().rstrip(':')\n",
        "                        field_value = value.text.strip()\n",
        "                        film_data[field_name] = field_value\n",
        "                synopsis_elem = soup.find('div', class_='field-name-body')\n",
        "                if synopsis_elem:\n",
        "                    film_data['synopsis'] = synopsis_elem.text.strip()\n",
        "                return film_data\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for {film_url}: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    time.sleep(delay)\n",
        "        return {'film_url': film_url, 'error': 'Failed to scrape'}\n",
        "\n",
        "    print(\"\\nScraping film details...\")\n",
        "    df_urls = pd.read_csv('filmitalia_urls.csv')\n",
        "    films_data = []\n",
        "    for url in tqdm(df_urls['film_url']):\n",
        "        films_data.append(scrape_film_details(url))\n",
        "        time.sleep(1)\n",
        "\n",
        "    df = pd.DataFrame(films_data)\n",
        "    df.to_csv('filmitalia_raw.csv', index=False)\n",
        "    print(f\"Scraped details for {len(df)} films\")\n",
        "\n",
        "else:\n",
        "    print(\"Scraping inactive. Set RUN_SCRAPING = True to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. DATA CLEANUP\n",
        "\n",
        "**⚠️ Set `RUN_CLEANUP = True` to execute**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_CLEANUP = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [OPTIONAL LOAD] df = pd.read_csv('filmitalia_raw.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_CLEANUP:\n",
        "    print(\"Loading raw scraped data...\")\n",
        "    df = pd.read_csv('filmitalia_raw.csv')\n",
        "\n",
        "    def parse_duration(duration_str):\n",
        "        if pd.isna(duration_str):\n",
        "            return None\n",
        "        duration_str = str(duration_str).strip()\n",
        "        numbers = re.findall(r'\\d+', duration_str)\n",
        "        if not numbers:\n",
        "            return None\n",
        "        if 'h' in duration_str.lower() or 'ore' in duration_str.lower():\n",
        "            return int(numbers[0]) * 60 + int(numbers[1]) if len(numbers) >= 2 else int(numbers[0]) * 60\n",
        "        return int(numbers[0])\n",
        "\n",
        "    if 'duration' in df.columns:\n",
        "        df['duration_minutes'] = df['duration'].apply(parse_duration)\n",
        "\n",
        "    if 'wikidata_id' in df.columns:\n",
        "        def get_wikidata_summary(qid):\n",
        "            if pd.isna(qid):\n",
        "                return None\n",
        "            try:\n",
        "                url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
        "                response = requests.get(url, timeout=10)\n",
        "                data = response.json()\n",
        "                entity = data.get('entities', {}).get(qid, {})\n",
        "                return entity.get('descriptions', {}).get('it', {}).get('value')\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "        print(\"Enriching with Wikidata summaries...\")\n",
        "        tqdm.pandas()\n",
        "        df['wikidata_summary'] = df['wikidata_id'].progress_apply(get_wikidata_summary)\n",
        "\n",
        "    df.to_csv('filmitalia_cleaned.csv', index=False)\n",
        "    print(f\"Cleaned data saved: {len(df)} films\")\n",
        "else:\n",
        "    print(\"Cleanup inactive. Set RUN_CLEANUP = True to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. LOCATION EXTRACTION (spaCy NER)\n",
        "\n",
        "Extracts from **BOTH** `synopsis` and `wikipedia_summary`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from GitHub\n",
        "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/filmitalia_details_enriched.csv\"\n",
        "df = pd.read_csv(GITHUB_DATA_URL)\n",
        "print(f\"Loaded {len(df)} films from GitHub\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_locations(text):\n",
        "    \"\"\"Extract location entities (GPE, LOC) from Italian text using spaCy.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    doc = nlp(str(text))\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]\n",
        "    return list(set(locations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load stopwords\n",
        "STOPWORDS_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/stopwords.txt\"\n",
        "response = requests.get(STOPWORDS_URL)\n",
        "response.raise_for_status()\n",
        "stopwords = [w.strip() for w in response.text.split(\",\") if w.strip()]\n",
        "print(f\"Loaded {len(stopwords)} stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MIN_LENGTH = 3\n",
        "MIN_ACRONYM_LENGTH = 5\n",
        "ITALIAN_PREFIXES = r'^(di|a|da|in|nel|nell|nella|negli|per|verso|il|la|lo|le|i|gli|del|della|dei|degli|al|alla|ai|agli)\\s+'\n",
        "\n",
        "def clean_locations(location_list):\n",
        "    \"\"\"Remove stopwords, invalid entries, and Italian prefixes.\"\"\"\n",
        "    if not location_list or not isinstance(location_list, list):\n",
        "        return []\n",
        "    cleaned = []\n",
        "    for loc in location_list:\n",
        "        if loc in stopwords or len(loc) < MIN_LENGTH or (loc.isupper() and len(loc) < MIN_ACRONYM_LENGTH):\n",
        "            continue\n",
        "        loc = re.sub(ITALIAN_PREFIXES, '', loc, flags=re.IGNORECASE)\n",
        "        if match := re.match(r'^[a-z]+([A-Z].*)$', loc):\n",
        "            loc = match.group(1)\n",
        "        if loc:\n",
        "            cleaned.append(loc)\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load silver standard\n",
        "SILVER_STANDARD_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/silver_standard.txt\"\n",
        "response = requests.get(SILVER_STANDARD_URL)\n",
        "response.raise_for_status()\n",
        "silver_standard = set(line.strip() for line in response.text.splitlines() if line.strip())\n",
        "print(f\"Loaded {len(silver_standard)} locations from silver_standard.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_discrepancies(entities, standards_set, discrepancy_collector):\n",
        "    \"\"\"Collect entities NOT in the standards set for manual review.\"\"\"\n",
        "    if not isinstance(entities, list):\n",
        "        return\n",
        "    for ent in entities:\n",
        "        if ent not in standards_set:\n",
        "            discrepancy_collector.add(ent)\n",
        "\n",
        "def filter_by_silver_standard(entities):\n",
        "    \"\"\"Keep only entities in silver standard.\"\"\"\n",
        "    if not isinstance(entities, list):\n",
        "        return []\n",
        "    return [ent for ent in entities if ent in silver_standard]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Extract from SYNOPSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting locations from SYNOPSIS...\")\n",
        "tqdm.pandas()\n",
        "df['locations_synopsis'] = df['synopsis'].progress_apply(extract_locations)\n",
        "\n",
        "raw_count = sum(len(x) for x in df['locations_synopsis'])\n",
        "print(f\"Raw extraction: {raw_count} mentions\")\n",
        "\n",
        "df['locations_synopsis'] = df['locations_synopsis'].apply(clean_locations)\n",
        "clean_count = sum(len(x) for x in df['locations_synopsis'])\n",
        "print(f\"After cleaning: {clean_count} mentions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check discrepancies BEFORE filtering\n",
        "discrepancies_synopsis = set()\n",
        "df['locations_synopsis'].apply(\n",
        "    lambda x: collect_discrepancies(x, silver_standard, discrepancies_synopsis)\n",
        ")\n",
        "print(f\"\\nSYNOPSIS: {len(discrepancies_synopsis)} unique elements NOT in silver_standard.txt\")\n",
        "if discrepancies_synopsis:\n",
        "    print(\"\\n--- Elements in locations_synopsis NOT in silver_standard ---\")\n",
        "    for element in sorted(list(discrepancies_synopsis)):\n",
        "        print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply silver standard filter\n",
        "df['locations_synopsis'] = df['locations_synopsis'].apply(filter_by_silver_standard)\n",
        "final_count = sum(len(x) for x in df['locations_synopsis'])\n",
        "print(f\"After silver standard filter: {final_count} mentions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Extract from WIKIPEDIA_SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting locations from WIKIPEDIA_SUMMARY...\")\n",
        "tqdm.pandas()\n",
        "df['locations_wiki'] = df['wikipedia_summary'].progress_apply(extract_locations)\n",
        "\n",
        "raw_count = sum(len(x) for x in df['locations_wiki'])\n",
        "print(f\"Raw extraction: {raw_count} mentions\")\n",
        "\n",
        "df['locations_wiki'] = df['locations_wiki'].apply(clean_locations)\n",
        "clean_count = sum(len(x) for x in df['locations_wiki'])\n",
        "print(f\"After cleaning: {clean_count} mentions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check discrepancies BEFORE filtering\n",
        "discrepancies_wiki = set()\n",
        "df['locations_wiki'].apply(\n",
        "    lambda x: collect_discrepancies(x, silver_standard, discrepancies_wiki)\n",
        ")\n",
        "print(f\"\\nWIKIPEDIA: {len(discrepancies_wiki)} unique elements NOT in silver_standard.txt\")\n",
        "if discrepancies_wiki:\n",
        "    print(\"\\n--- Elements in locations_wiki NOT in silver_standard ---\")\n",
        "    for element in sorted(list(discrepancies_wiki)):\n",
        "        print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply silver standard filter\n",
        "df['locations_wiki'] = df['locations_wiki'].apply(filter_by_silver_standard)\n",
        "final_count = sum(len(x) for x in df['locations_wiki'])\n",
        "print(f\"After silver standard filter: {final_count} mentions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "synopsis_locs = [loc for locs in df['locations_synopsis'] for loc in locs]\n",
        "wiki_locs = [loc for locs in df['locations_wiki'] for loc in locs]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOCATION EXTRACTION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Synopsis:  {len(synopsis_locs)} mentions, {len(set(synopsis_locs))} unique\")\n",
        "print(f\"Wikipedia: {len(wiki_locs)} mentions, {len(set(wiki_locs))} unique\")\n",
        "print(f\"Combined unique: {len(set(synopsis_locs) | set(wiki_locs))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SAVE]\n",
        "df.to_csv('filmitalia_locations.csv', index=False)\n",
        "print(\"Saved: filmitalia_locations.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. GEOCODING (Nominatim)\n",
        "\n",
        "- `USE_GEOCACHE = True` → Load from cache\n",
        "- `USE_GEOCACHE = False` → Geocode from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [OPTIONAL LOAD]\n",
        "# df = pd.read_csv('filmitalia_locations.csv')\n",
        "# df['locations_synopsis'] = df['locations_synopsis'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x != '[]' else [])\n",
        "# df['locations_wiki'] = df['locations_wiki'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x != '[]' else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USE_GEOCACHE = False\n",
        "GEOCACHE_FILE = 'geocache.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def geocode_location(location_name):\n",
        "    \"\"\"Geocode a location using Nominatim.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            \"https://nominatim.openstreetmap.org/search\",\n",
        "            params={'q': location_name, 'format': 'json', 'limit': 1},\n",
        "            headers={'User-Agent': 'ItalianCinemaResearch/1.0'},\n",
        "            timeout=10\n",
        "        )\n",
        "        data = response.json()\n",
        "        if data:\n",
        "            return (float(data[0]['lat']), float(data[0]['lon']))\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {location_name}: {e}\")\n",
        "    return (None, None)\n",
        "\n",
        "def load_geocache(filepath):\n",
        "    cache = {}\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            cache_df = pd.read_csv(filepath)\n",
        "            for _, row in cache_df.iterrows():\n",
        "                loc = row.get('location', row.iloc[0] if len(row) > 0 else None)\n",
        "                lat = row.get('latitude', row.iloc[1] if len(row) > 1 else None)\n",
        "                lon = row.get('longitude', row.iloc[2] if len(row) > 2 else None)\n",
        "                if loc and pd.notna(lat) and pd.notna(lon):\n",
        "                    cache[loc] = [float(lat), float(lon)]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load geocache: {e}\")\n",
        "    return cache\n",
        "\n",
        "def save_geocache(cache, filepath):\n",
        "    rows = [{'location': k, 'latitude': v[0], 'longitude': v[1]} \n",
        "            for k, v in cache.items() if v]\n",
        "    pd.DataFrame(rows).to_csv(filepath, index=False)\n",
        "    print(f\"Saved {len(rows)} locations to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect ALL unique locations from BOTH sources\n",
        "all_unique_locations = set()\n",
        "df['locations_synopsis'].apply(lambda x: all_unique_locations.update(x) if isinstance(x, list) else None)\n",
        "synopsis_unique = len(all_unique_locations)\n",
        "df['locations_wiki'].apply(lambda x: all_unique_locations.update(x) if isinstance(x, list) else None)\n",
        "wiki_added = len(all_unique_locations) - synopsis_unique\n",
        "\n",
        "print(f\"Unique from synopsis: {synopsis_unique}\")\n",
        "print(f\"Additional from wikipedia: {wiki_added}\")\n",
        "print(f\"Total to geocode: {len(all_unique_locations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_GEOCACHE and os.path.exists(GEOCACHE_FILE):\n",
        "    print(f\"Loading geocache from {GEOCACHE_FILE}...\")\n",
        "    location_coords = load_geocache(GEOCACHE_FILE)\n",
        "    print(f\"Loaded {len(location_coords)} cached coordinates\")\n",
        "    to_geocode = all_unique_locations - set(location_coords.keys())\n",
        "    print(f\"Need to geocode {len(to_geocode)} new locations\")\n",
        "else:\n",
        "    location_coords = {}\n",
        "    to_geocode = all_unique_locations\n",
        "    print(f\"Starting fresh geocoding for {len(to_geocode)} locations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if to_geocode:\n",
        "    print(f\"\\nGeocoding {len(to_geocode)} locations...\")\n",
        "    new_coords = {}\n",
        "    for location in tqdm(to_geocode):\n",
        "        lat, lon = geocode_location(location)\n",
        "        new_coords[location] = [lat, lon] if lat and lon else None\n",
        "        time.sleep(1)\n",
        "\n",
        "    location_coords.update(new_coords)\n",
        "    save_geocache(location_coords, GEOCACHE_FILE)\n",
        "\n",
        "success_count = sum(1 for v in location_coords.values() if v)\n",
        "print(f\"\\nSuccessfully geocoded: {success_count}/{len(all_unique_locations)} locations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_coordinates(location_list):\n",
        "    if not isinstance(location_list, list):\n",
        "        return []\n",
        "    return [location_coords.get(loc) for loc in location_list if location_coords.get(loc)]\n",
        "\n",
        "df['coordinates_synopsis'] = df['locations_synopsis'].apply(get_coordinates)\n",
        "df['coordinates_wiki'] = df['locations_wiki'].apply(get_coordinates)\n",
        "\n",
        "print(\"Sample (synopsis):\")\n",
        "print(df[['original_title', 'locations_synopsis', 'coordinates_synopsis']].head(3))\n",
        "print(\"\\nSample (wikipedia):\")\n",
        "print(df[['original_title', 'locations_wiki', 'coordinates_wiki']].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [SAVE]\n",
        "df.to_csv('filmitalia_geocoded.csv', index=False)\n",
        "print(\"Saved: filmitalia_geocoded.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. VISUALIZATION & ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [OPTIONAL LOAD]\n",
        "# df = pd.read_csv('filmitalia_geocoded.csv')\n",
        "# for col in ['locations_synopsis', 'locations_wiki', 'coordinates_synopsis', 'coordinates_wiki']:\n",
        "#     df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x != '[]' else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import folium\n",
        "from folium.plugins import HeatMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for visualizations\n",
        "synopsis_locs = [loc for locs in df['locations_synopsis'] for loc in (locs if isinstance(locs, list) else [])]\n",
        "wiki_locs = [loc for locs in df['locations_wiki'] for loc in (locs if isinstance(locs, list) else [])]\n",
        "\n",
        "synopsis_counts = Counter(synopsis_locs)\n",
        "wiki_counts = Counter(wiki_locs)\n",
        "\n",
        "print(f\"Total synopsis mentions: {len(synopsis_locs)}, unique: {len(set(synopsis_locs))}\")\n",
        "print(f\"Total wiki mentions: {len(wiki_locs)}, unique: {len(set(wiki_locs))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Venn-style Overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synopsis_set = set(synopsis_locs)\n",
        "wiki_set = set(wiki_locs)\n",
        "\n",
        "only_synopsis = synopsis_set - wiki_set\n",
        "only_wiki = wiki_set - synopsis_set\n",
        "both = synopsis_set & wiki_set\n",
        "\n",
        "print(f\"Only in synopsis: {len(only_synopsis)}\")\n",
        "print(f\"Only in wiki: {len(only_wiki)}\")\n",
        "print(f\"In both: {len(both)}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.pie([len(only_synopsis), len(only_wiki), len(both)],\n",
        "       labels=[f'Synopsis only\\n({len(only_synopsis)})',\n",
        "               f'Wiki only\\n({len(only_wiki)})',\n",
        "               f'Both\\n({len(both)})'],\n",
        "       colors=['steelblue', 'darkorange', 'purple'],\n",
        "       autopct='%1.1f%%')\n",
        "ax.set_title('Location Overlap between Sources')\n",
        "plt.savefig('location_overlap.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Top Locations Comparison (Percentages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_synopsis = len(synopsis_locs)\n",
        "total_wiki = len(wiki_locs)\n",
        "\n",
        "all_top = set([x[0] for x in synopsis_counts.most_common(20)] +\n",
        "              [x[0] for x in wiki_counts.most_common(20)])\n",
        "\n",
        "compare_df = pd.DataFrame({\n",
        "    'location': list(all_top),\n",
        "    'synopsis': [synopsis_counts.get(loc, 0) / total_synopsis * 100 for loc in all_top],\n",
        "    'wiki': [wiki_counts.get(loc, 0) / total_wiki * 100 for loc in all_top]\n",
        "}).sort_values('synopsis', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "y = range(len(compare_df))\n",
        "ax.barh([i - 0.2 for i in y], compare_df['synopsis'], 0.4, label='Synopsis', color='steelblue')\n",
        "ax.barh([i + 0.2 for i in y], compare_df['wiki'], 0.4, label='Wikipedia', color='darkorange')\n",
        "ax.set_yticks(y)\n",
        "ax.set_yticklabels(compare_df['location'])\n",
        "ax.set_xlabel('Percentage of mentions')\n",
        "ax.set_title('Top Locations Comparison (% of total mentions)')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_comparison_pct.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Frequency Correlation Scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_locs = list(both)\n",
        "x = [synopsis_counts[loc] / total_synopsis * 100 for loc in common_locs]\n",
        "y = [wiki_counts[loc] / total_wiki * 100 for loc in common_locs]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.scatter(x, y, alpha=0.6)\n",
        "for i, loc in enumerate(common_locs):\n",
        "    if x[i] > 2 or y[i] > 2:\n",
        "        ax.annotate(loc, (x[i], y[i]), fontsize=8)\n",
        "ax.set_xlabel('Synopsis frequency (%)')\n",
        "ax.set_ylabel('Wiki frequency (%)')\n",
        "ax.set_title('Location frequency correlation (% of total mentions)')\n",
        "ax.plot([0, max(x) if x else 1], [0, max(x) if x else 1], 'k--', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_correlation.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Donut Charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_donut_data(counts, total):\n",
        "    pct = {loc: count / total * 100 for loc, count in counts.items()}\n",
        "    top = {loc: p for loc, p in pct.items() if p >= 1}\n",
        "    others_pct = sum(p for p in pct.values() if p < 1)\n",
        "    return top, others_pct\n",
        "\n",
        "synopsis_top, synopsis_others = prepare_donut_data(synopsis_counts, len(synopsis_locs))\n",
        "wiki_top, wiki_others = prepare_donut_data(wiki_counts, len(wiki_locs))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Synopsis donut\n",
        "labels1 = list(synopsis_top.keys()) + ['Others (<1%)']\n",
        "sizes1 = list(synopsis_top.values()) + [synopsis_others]\n",
        "explode1 = [0.02] * len(sizes1)\n",
        "colors1 = plt.cm.Blues([0.3 + 0.5 * i / len(sizes1) for i in range(len(sizes1))])\n",
        "wedges1, texts1, autotexts1 = ax1.pie(sizes1, labels=labels1, autopct='%1.1f%%',\n",
        "                                       explode=explode1, colors=colors1, pctdistance=0.75, rotatelabels=True)\n",
        "centre_circle1 = plt.Circle((0, 0), 0.50, fc='white')\n",
        "ax1.add_artist(centre_circle1)\n",
        "ax1.set_title('Synopsis locations')\n",
        "\n",
        "# Wiki donut\n",
        "labels2 = list(wiki_top.keys()) + ['Others (<1%)']\n",
        "sizes2 = list(wiki_top.values()) + [wiki_others]\n",
        "explode2 = [0.02] * len(sizes2)\n",
        "colors2 = plt.cm.Oranges([0.3 + 0.5 * i / len(sizes2) for i in range(len(sizes2))])\n",
        "wedges2, texts2, autotexts2 = ax2.pie(sizes2, labels=labels2, autopct='%1.1f%%',\n",
        "                                       explode=explode2, colors=colors2, pctdistance=0.75, rotatelabels=True)\n",
        "centre_circle2 = plt.Circle((0, 0), 0.50, fc='white')\n",
        "ax2.add_artist(centre_circle2)\n",
        "ax2.set_title('Wikipedia locations')\n",
        "\n",
        "plt.suptitle('Location distribution (≥1% labelled)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_donuts.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Interactive Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_locations_map(df, source='both'):\n",
        "    coords_with_labels = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        if source in ['synopsis', 'both']:\n",
        "            if isinstance(row['locations_synopsis'], list) and isinstance(row['coordinates_synopsis'], list):\n",
        "                for loc, coord in zip(row['locations_synopsis'], row['coordinates_synopsis']):\n",
        "                    if coord and coord[0] and coord[1]:\n",
        "                        coords_with_labels.append((coord[0], coord[1], loc, 'synopsis'))\n",
        "        if source in ['wiki', 'both']:\n",
        "            if isinstance(row['locations_wiki'], list) and isinstance(row['coordinates_wiki'], list):\n",
        "                for loc, coord in zip(row['locations_wiki'], row['coordinates_wiki']):\n",
        "                    if coord and coord[0] and coord[1]:\n",
        "                        coords_with_labels.append((coord[0], coord[1], loc, 'wiki'))\n",
        "\n",
        "    print(f\"Plotting {len(coords_with_labels)} location instances\")\n",
        "\n",
        "    m = folium.Map(location=[42.5, 12.5], zoom_start=6)\n",
        "    heat_data = [[lat, lon] for lat, lon, _, _ in coords_with_labels]\n",
        "    HeatMap(heat_data, radius=15, blur=25, max_zoom=13).add_to(m)\n",
        "\n",
        "    unique_coords = {}\n",
        "    for lat, lon, label, src in coords_with_labels:\n",
        "        key = (round(lat, 4), round(lon, 4))\n",
        "        if key not in unique_coords:\n",
        "            unique_coords[key] = {'labels': [], 'sources': set()}\n",
        "        unique_coords[key]['labels'].append(label)\n",
        "        unique_coords[key]['sources'].add(src)\n",
        "\n",
        "    for (lat, lon), data in unique_coords.items():\n",
        "        color = 'red' if 'synopsis' in data['sources'] else 'blue'\n",
        "        if len(data['sources']) > 1:\n",
        "            color = 'purple'\n",
        "        folium.CircleMarker(\n",
        "            location=[lat, lon],\n",
        "            radius=4,\n",
        "            popup=f\"<b>{', '.join(set(data['labels']))}</b><br>Count: {len(data['labels'])}<br>Source: {', '.join(data['sources'])}\",\n",
        "            tooltip=data['labels'][0],\n",
        "            color=color,\n",
        "            fill=True,\n",
        "            fillOpacity=0.7\n",
        "        ).add_to(m)\n",
        "\n",
        "    print(f\"Added {len(unique_coords)} unique markers\")\n",
        "    return m\n",
        "\n",
        "map_obj = create_locations_map(df, source='both')\n",
        "map_obj.save('locations_map.html')\n",
        "print(\"Saved: locations_map.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "map_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.6 Final Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create flattened location table\n",
        "location_data = []\n",
        "for _, row in df.iterrows():\n",
        "    title = row.get('original_title', row.get('title', 'Unknown'))\n",
        "    year = row.get('year')\n",
        "    \n",
        "    if isinstance(row['locations_synopsis'], list) and isinstance(row['coordinates_synopsis'], list):\n",
        "        for loc, coord in zip(row['locations_synopsis'], row['coordinates_synopsis']):\n",
        "            if coord:\n",
        "                location_data.append({\n",
        "                    'film_title': title, 'year': year, 'location': loc,\n",
        "                    'latitude': coord[0], 'longitude': coord[1], 'source': 'synopsis'\n",
        "                })\n",
        "    \n",
        "    if isinstance(row['locations_wiki'], list) and isinstance(row['coordinates_wiki'], list):\n",
        "        for loc, coord in zip(row['locations_wiki'], row['coordinates_wiki']):\n",
        "            if coord:\n",
        "                location_data.append({\n",
        "                    'film_title': title, 'year': year, 'location': loc,\n",
        "                    'latitude': coord[0], 'longitude': coord[1], 'source': 'wikipedia'\n",
        "                })\n",
        "\n",
        "df_locations = pd.DataFrame(location_data)\n",
        "print(f\"Flattened table: {len(df_locations)} entries\")\n",
        "print(f\"  - From synopsis: {len(df_locations[df_locations['source']=='synopsis'])}\")\n",
        "print(f\"  - From wikipedia: {len(df_locations[df_locations['source']=='wikipedia'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grouped location summary\n",
        "locations_summary = df_locations.groupby(['location', 'latitude', 'longitude']).size().reset_index(name='count')\n",
        "locations_summary = locations_summary.sort_values('count', ascending=False)\n",
        "print(f\"\\nUnique location-coordinate pairs: {len(locations_summary)}\")\n",
        "print(locations_summary.head(20).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [FINAL SAVES]\n",
        "df.to_csv('database.csv', index=False)\n",
        "df_locations.to_csv('locations_flat.csv', index=False)\n",
        "locations_summary.to_csv('locations_coordinates.csv', index=False)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL OUTPUTS SAVED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"database.csv          - {len(df)} films with all data\")\n",
        "print(f\"locations_flat.csv    - {len(df_locations)} film-location pairs\")\n",
        "print(f\"locations_coordinates.csv - {len(locations_summary)} unique locations with counts\")\n",
        "print(f\"locations_map.html    - Interactive map\")\n",
        "print(f\"geocache.txt          - Coordinate cache for reuse\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
