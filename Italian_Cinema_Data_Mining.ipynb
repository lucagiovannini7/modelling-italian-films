{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Italian Cinema Data Mining & Geocoding\n",
        "\n",
        "A comprehensive pipeline for extracting, geocoding, and analyzing locations mentioned in Italian film synopses and Wikipedia summaries.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "| Section | Description | Output |\n",
        "|---------|-------------|--------|\n",
        "| **1. Setup** | Install dependencies, load libraries | \u2014 |\n",
        "| **2. Scraping** *(optional)* | Extract film data from filmitalia.org | `filmitalia_raw.csv` |\n",
        "| **3. Cleanup** *(optional)* | Clean and enrich data with Wikidata | `filmitalia_cleaned.csv` |\n",
        "| **4. Location Extraction** | NER-based extraction from synopsis & wikipedia | `filmitalia_locations.csv` |\n",
        "| \u2014 4.1 Synopsis | Extract from `synopsis` column | \u2014 |\n",
        "| \u2014 4.2 Wikipedia | Extract from `wikipedia_summary` column | \u2014 |\n",
        "| **5. Geocoding** | Geocode locations via Nominatim API | `filmitalia_geocoded.csv` |\n",
        "| **6. Semantic Analysis** | Lexicon-based spatial semantics | \u2014 |\n",
        "| \u2014 6.1 Settlement/Nature | Urban vs. natural landscape terms | \u2014 |\n",
        "| \u2014 6.2 Mobility/Static | Movement vs. sedentary lexicon | \u2014 |\n",
        "| **7. Visualization & Analysis** | Maps, charts, and comparative analysis | `database.csv` |\n",
        "| \u2014 7.1 Location Overlap | Venn-style overlap between sources | \u2014 |\n",
        "| \u2014 7.2 Frequency Analysis | Top locations, correlation plots | \u2014 |\n",
        "| \u2014 7.3 Interactive Map | Folium heatmap | `locations_map.html` |\n",
        "| \u2014 7.4 Final Outputs | Export all datasets | Multiple CSVs |\n",
        "\n",
        "---\n",
        "\n",
        "**Key output columns:** `locations_synopsis`, `coordinates_synopsis`, `locations_wiki`, `coordinates_wiki`, `settlement_*`, `nature_*`, `stanziale_*`, `movimento_*`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load('it_core_news_sm')\n",
        "except:\n",
        "    !python -m spacy download it_core_news_sm\n",
        "    nlp = spacy.load('it_core_news_sm')\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## 2. Web Scraping *(Optional)*\n\n**\u26a0\ufe0f Set `RUN_SCRAPING = True` to execute.** This section scrapes filmitalia.org and takes several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_SCRAPING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_SCRAPING:\n\n    def scrape_film_urls(page_num, retries=3, delay=2):\n        url = f\"https://filmitalia.org/it/film/pag-{page_num}/\"\n        for attempt in range(retries):\n            try:\n                response = requests.get(url, timeout=10)\n                response.raise_for_status()\n                soup = BeautifulSoup(response.content, 'html.parser')\n                film_links = []\n                for link in soup.find_all('a', href=True):\n                    href = link['href']\n                    if '/it/film/' in href and href.endswith('/'):\n                        if href.startswith('/'):\n                            href = 'https://filmitalia.org' + href\n                        film_links.append(href)\n                return list(set(film_links))\n            except Exception as e:\n                print(f\"Attempt {attempt + 1} failed for page {page_num}: {e}\")\n                if attempt < retries - 1:\n                    time.sleep(delay)\n        return []\n\n    print(\"Scraping film URLs...\")\n    all_film_urls = []\n    for page in tqdm(range(1, 500)):\n        urls = scrape_film_urls(page)\n        if not urls:\n            print(f\"No URLs found on page {page}. Stopping.\")\n            break\n        all_film_urls.extend(urls)\n        time.sleep(1)\n\n    df_urls = pd.DataFrame({'film_url': list(set(all_film_urls))})\n    df_urls.to_csv('filmitalia_urls.csv', index=False)\n    print(f\"Scraped {len(df_urls)} unique film URLs\")\n\n    def scrape_film_details(film_url, retries=3, delay=2):\n        for attempt in range(retries):\n            try:\n                response = requests.get(film_url, timeout=10)\n                response.raise_for_status()\n                soup = BeautifulSoup(response.content, 'html.parser')\n                film_data = {'film_url': film_url}\n                title_elem = soup.find('h1')\n                film_data['title'] = title_elem.text.strip() if title_elem else None\n                for row in soup.find_all('div', class_='field'):\n                    label = row.find('div', class_='field-label')\n                    value = row.find('div', class_='field-items')\n                    if label and value:\n                        field_name = label.text.strip().rstrip(':')\n                        field_value = value.text.strip()\n                        film_data[field_name] = field_value\n                synopsis_elem = soup.find('div', class_='field-name-body')\n                if synopsis_elem:\n                    film_data['synopsis'] = synopsis_elem.text.strip()\n                return film_data\n            except Exception as e:\n                print(f\"Attempt {attempt + 1} failed for {film_url}: {e}\")\n                if attempt < retries - 1:\n                    time.sleep(delay)\n        return {'film_url': film_url, 'error': 'Failed to scrape'}\n\n    print(\"\\nScraping film details...\")\n    df_urls = pd.read_csv('filmitalia_urls.csv')\n    films_data = []\n    for url in tqdm(df_urls['film_url']):\n        films_data.append(scrape_film_details(url))\n        time.sleep(1)\n\n    df = pd.DataFrame(films_data)\n    df.to_csv('filmitalia_raw.csv', index=False)\n    print(f\"Scraped details for {len(df)} films\")\n\nelse:\n    print(\"Scraping inactive. Set RUN_SCRAPING = True to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## 3. Data Cleanup *(Optional)*\n\n**\u26a0\ufe0f Set `RUN_CLEANUP = True` to execute.** Parses duration, enriches with Wikidata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_CLEANUP = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [OPTIONAL LOAD] df = pd.read_csv('filmitalia_raw.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_CLEANUP:\n    print(\"Loading raw scraped data...\")\n    df = pd.read_csv('filmitalia_raw.csv')\n\n    def parse_duration(duration_str):\n        if pd.isna(duration_str):\n            return None\n        duration_str = str(duration_str).strip()\n        numbers = re.findall(r'\\d+', duration_str)\n        if not numbers:\n            return None\n        if 'h' in duration_str.lower() or 'ore' in duration_str.lower():\n            return int(numbers[0]) * 60 + int(numbers[1]) if len(numbers) >= 2 else int(numbers[0]) * 60\n        return int(numbers[0])\n\n    if 'duration' in df.columns:\n        df['duration_minutes'] = df['duration'].apply(parse_duration)\n\n    if 'wikidata_id' in df.columns:\n        def get_wikidata_summary(qid):\n            if pd.isna(qid):\n                return None\n            try:\n                url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n                response = requests.get(url, timeout=10)\n                data = response.json()\n                entity = data.get('entities', {}).get(qid, {})\n                return entity.get('descriptions', {}).get('it', {}).get('value')\n            except:\n                return None\n\n        print(\"Enriching with Wikidata summaries...\")\n        tqdm.pandas()\n        df['wikidata_summary'] = df['wikidata_id'].progress_apply(get_wikidata_summary)\n\n    df.to_csv('filmitalia_cleaned.csv', index=False)\n    print(f\"Cleaned data saved: {len(df)} films\")\nelse:\n    print(\"Cleanup inactive. Set RUN_CLEANUP = True to execute.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## 4. Location Extraction (spaCy NER)\n\nExtracts geographic entities (`GPE`, `LOC`) from both `synopsis` and `wikipedia_summary` columns.\n\n- Applies stopword filtering\n- Removes Italian article prefixes\n- Validates against silver standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from GitHub repository\n",
        "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/filmitalia_details_enriched.csv\"\n",
        "df = pd.read_csv(GITHUB_DATA_URL)\n",
        "print(f\"Loaded {len(df)} films from GitHub\")\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core NER extraction and cleaning functions\n",
        "\n",
        "def extract_locations(text):\n",
        "    \"\"\"Extract location entities (GPE, LOC) from Italian text using spaCy.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    doc = nlp(str(text))\n",
        "    return list(set(ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']))\n",
        "\n",
        "# Load stopwords and silver standard\n",
        "STOPWORDS_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/stopwords.txt\"\n",
        "SILVER_STANDARD_URL = \"https://raw.githubusercontent.com/lucagiovannini7/modelling-italian-films/refs/heads/main/silver_standard.txt\"\n",
        "\n",
        "stopwords = [w.strip() for w in requests.get(STOPWORDS_URL).text.split(\",\") if w.strip()]\n",
        "silver_standard = set(line.strip() for line in requests.get(SILVER_STANDARD_URL).text.splitlines() if line.strip())\n",
        "print(f\"Loaded {len(stopwords)} stopwords, {len(silver_standard)} silver standard locations\")\n",
        "\n",
        "# Cleaning parameters\n",
        "MIN_LENGTH = 3\n",
        "MIN_ACRONYM_LENGTH = 5\n",
        "ITALIAN_PREFIXES = r'^(di|a|da|in|nel|nell|nella|negli|per|verso|il|la|lo|le|i|gli|del|della|dei|degli|al|alla|ai|agli)\\s+'\n",
        "\n",
        "def clean_locations(location_list):\n",
        "    \"\"\"Remove stopwords, invalid entries, and Italian prefixes.\"\"\"\n",
        "    if not location_list or not isinstance(location_list, list):\n",
        "        return []\n",
        "    cleaned = []\n",
        "    for loc in location_list:\n",
        "        if loc in stopwords or len(loc) < MIN_LENGTH or (loc.isupper() and len(loc) < MIN_ACRONYM_LENGTH):\n",
        "            continue\n",
        "        loc = re.sub(ITALIAN_PREFIXES, '', loc, flags=re.IGNORECASE)\n",
        "        if match := re.match(r'^[a-z]+([A-Z].*)$', loc):\n",
        "            loc = match.group(1)\n",
        "        if loc:\n",
        "            cleaned.append(loc)\n",
        "    return cleaned\n",
        "\n",
        "def filter_by_silver_standard(entities):\n",
        "    \"\"\"Keep only entities in silver standard.\"\"\"\n",
        "    return [ent for ent in entities if ent in silver_standard] if isinstance(entities, list) else []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Extract from Synopsis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting locations from SYNOPSIS...\")\n",
        "tqdm.pandas()\n",
        "df['locations_synopsis'] = df['synopsis'].progress_apply(extract_locations)\n",
        "print(f\"Raw extraction: {sum(len(x) for x in df['locations_synopsis'])} mentions\")\n",
        "\n",
        "df['locations_synopsis'] = df['locations_synopsis'].apply(clean_locations)\n",
        "print(f\"After cleaning: {sum(len(x) for x in df['locations_synopsis'])} mentions\")\n",
        "\n",
        "df['locations_synopsis'] = df['locations_synopsis'].apply(filter_by_silver_standard)\n",
        "print(f\"After silver standard filter: {sum(len(x) for x in df['locations_synopsis'])} mentions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Extract from Wikipedia Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Extracting locations from WIKIPEDIA_SUMMARY...\")\n",
        "tqdm.pandas()\n",
        "df['locations_wiki'] = df['wikipedia_summary'].progress_apply(extract_locations)\n",
        "print(f\"Raw extraction: {sum(len(x) for x in df['locations_wiki'])} mentions\")\n",
        "\n",
        "df['locations_wiki'] = df['locations_wiki'].apply(clean_locations)\n",
        "print(f\"After cleaning: {sum(len(x) for x in df['locations_wiki'])} mentions\")\n",
        "\n",
        "df['locations_wiki'] = df['locations_wiki'].apply(filter_by_silver_standard)\n",
        "print(f\"After silver standard filter: {sum(len(x) for x in df['locations_wiki'])} mentions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Location extraction summary\n",
        "synopsis_locs = [loc for locs in df['locations_synopsis'] for loc in locs]\n",
        "wiki_locs = [loc for locs in df['locations_wiki'] for loc in locs]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOCATION EXTRACTION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Synopsis:  {len(synopsis_locs)} mentions, {len(set(synopsis_locs))} unique\")\n",
        "print(f\"Wikipedia: {len(wiki_locs)} mentions, {len(set(wiki_locs))} unique\")\n",
        "print(f\"Combined unique: {len(set(synopsis_locs) | set(wiki_locs))}\")\n",
        "\n",
        "# Save checkpoint\n",
        "df.to_csv('filmitalia_locations.csv', index=False)\n",
        "print(\"\\nSaved: filmitalia_locations.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Geocoding (Nominatim)\n",
        "\n",
        "Geocode extracted locations using OpenStreetMap's Nominatim API.\n",
        "\n",
        "- `USE_GEOCACHE = True` \u2192 Load from cache file\n",
        "- `USE_GEOCACHE = False` \u2192 Geocode from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "USE_GEOCACHE = False\n",
        "GEOCACHE_FILE = 'geocache.txt'\n",
        "\n",
        "# [OPTIONAL LOAD]\n",
        "# df = pd.read_csv('filmitalia_locations.csv')\n",
        "# for col in ['locations_synopsis', 'locations_wiki']:\n",
        "#     df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x != '[]' else [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Geocoding functions\n",
        "def geocode_location(location_name):\n",
        "    \"\"\"Geocode a location using Nominatim.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            \"https://nominatim.openstreetmap.org/search\",\n",
        "            params={'q': location_name, 'format': 'json', 'limit': 1},\n",
        "            headers={'User-Agent': 'ItalianCinemaResearch/1.0'},\n",
        "            timeout=10\n",
        "        )\n",
        "        data = response.json()\n",
        "        if data:\n",
        "            return (float(data[0]['lat']), float(data[0]['lon']))\n",
        "    except Exception as e:\n",
        "        print(f\"Error geocoding {location_name}: {e}\")\n",
        "    return (None, None)\n",
        "\n",
        "def load_geocache(filepath):\n",
        "    cache = {}\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            cache_df = pd.read_csv(filepath)\n",
        "            for _, row in cache_df.iterrows():\n",
        "                loc = row.get('location', row.iloc[0] if len(row) > 0 else None)\n",
        "                lat = row.get('latitude', row.iloc[1] if len(row) > 1 else None)\n",
        "                lon = row.get('longitude', row.iloc[2] if len(row) > 2 else None)\n",
        "                if loc and pd.notna(lat) and pd.notna(lon):\n",
        "                    cache[loc] = [float(lat), float(lon)]\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load geocache: {e}\")\n",
        "    return cache\n",
        "\n",
        "def save_geocache(cache, filepath):\n",
        "    rows = [{'location': k, 'latitude': v[0], 'longitude': v[1]} for k, v in cache.items() if v]\n",
        "    pd.DataFrame(rows).to_csv(filepath, index=False)\n",
        "    print(f\"Saved {len(rows)} locations to {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all unique locations and geocode\n",
        "all_unique_locations = set()\n",
        "df['locations_synopsis'].apply(lambda x: all_unique_locations.update(x) if isinstance(x, list) else None)\n",
        "synopsis_unique = len(all_unique_locations)\n",
        "df['locations_wiki'].apply(lambda x: all_unique_locations.update(x) if isinstance(x, list) else None)\n",
        "print(f\"Unique from synopsis: {synopsis_unique}\")\n",
        "print(f\"Additional from wikipedia: {len(all_unique_locations) - synopsis_unique}\")\n",
        "print(f\"Total to geocode: {len(all_unique_locations)}\")\n",
        "\n",
        "# Load cache or start fresh\n",
        "if USE_GEOCACHE and os.path.exists(GEOCACHE_FILE):\n",
        "    location_coords = load_geocache(GEOCACHE_FILE)\n",
        "    print(f\"Loaded {len(location_coords)} cached coordinates\")\n",
        "    to_geocode = all_unique_locations - set(location_coords.keys())\n",
        "else:\n",
        "    location_coords = {}\n",
        "    to_geocode = all_unique_locations\n",
        "\n",
        "# Geocode new locations\n",
        "if to_geocode:\n",
        "    print(f\"\\nGeocoding {len(to_geocode)} locations...\")\n",
        "    for location in tqdm(to_geocode):\n",
        "        lat, lon = geocode_location(location)\n",
        "        location_coords[location] = [lat, lon] if lat and lon else None\n",
        "        time.sleep(1)\n",
        "    save_geocache(location_coords, GEOCACHE_FILE)\n",
        "\n",
        "print(f\"\\nSuccessfully geocoded: {sum(1 for v in location_coords.values() if v)}/{len(all_unique_locations)} locations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply coordinates to dataframe\n",
        "def get_coordinates(location_list):\n",
        "    if not isinstance(location_list, list):\n",
        "        return []\n",
        "    return [location_coords.get(loc) for loc in location_list if location_coords.get(loc)]\n",
        "\n",
        "df['coordinates_synopsis'] = df['locations_synopsis'].apply(get_coordinates)\n",
        "df['coordinates_wiki'] = df['locations_wiki'].apply(get_coordinates)\n",
        "\n",
        "print(\"Sample (synopsis):\")\n",
        "print(df[['original_title', 'locations_synopsis', 'coordinates_synopsis']].head(3))\n",
        "\n",
        "# Save checkpoint\n",
        "df.to_csv('filmitalia_geocoded.csv', index=False)\n",
        "print(\"\\nSaved: filmitalia_geocoded.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Semantic Analysis\n",
        "\n",
        "Lexicon-based analysis of spatial semantics in film descriptions. Two complementary approaches:\n",
        "\n",
        "1. **Settlement/Nature lexicon** \u2014 Identifies urban vs. natural landscape vocabulary\n",
        "2. **Mobility/Static lexicon** \u2014 Detects movement-related vs. sedentary verbs\n",
        "\n",
        "Both analyses are applied to `synopsis` and `wikipedia_summary` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# [OPTIONAL LOAD] - Start here if you have filmitalia_geocoded.csv\n",
        "# df = pd.read_csv('filmitalia_geocoded.csv', engine='python')\n",
        "# for col in ['locations_synopsis', 'locations_wiki', 'coordinates_synopsis', 'coordinates_wiki']:\n",
        "#     df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and x != '[]' else [])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Settlement/Nature Lexicon\n\nIdentifies mentions of urban settlements (citt\u00e0, quartiere, piazza...) vs. natural landscapes (mare, montagna, campagna...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define settlement/nature lexicons\n",
        "settlement_lexicon = [\n",
        "    'citt\u00e0', 'paese', 'paesino', 'provincia', 'regione', 'nazione',\n",
        "    'periferia', 'quartiere', 'rione', 'piazza', 'piazzetta',\n",
        "    'lungomare', 'porto', 'borgo', 'centro storico'\n",
        "]\n",
        "\n",
        "nature_lexicon = [\n",
        "    'mare', 'spiaggia', 'costa', 'isola', 'isole', 'laguna',\n",
        "    'montagna', 'montagne', 'collina', 'colline', 'valle', 'pianura',\n",
        "    'bosco', 'boschi', 'foresta', 'pineta',\n",
        "    'fiume', 'fiumi', 'lago', 'laghi', 'torrente',\n",
        "    'campagna', 'campagne', 'vigneto', 'entroterra'\n",
        "]\n",
        "\n",
        "def extract_lexicon(text, lexicon):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text_lower = text.lower()\n",
        "    return [term for term in lexicon if re.search(r'\\b' + term + r'\\b', text_lower)]\n",
        "\n",
        "# Extract from both columns\n",
        "df['settlement_synopsis'] = df['synopsis'].apply(lambda x: extract_lexicon(x, settlement_lexicon))\n",
        "df['nature_synopsis'] = df['synopsis'].apply(lambda x: extract_lexicon(x, nature_lexicon))\n",
        "df['settlement_wiki'] = df['wikipedia_summary'].apply(lambda x: extract_lexicon(x, settlement_lexicon))\n",
        "df['nature_wiki'] = df['wikipedia_summary'].apply(lambda x: extract_lexicon(x, nature_lexicon))\n",
        "\n",
        "# Statistics\n",
        "settlement_syn = [t for terms in df['settlement_synopsis'] for t in terms]\n",
        "nature_syn = [t for terms in df['nature_synopsis'] for t in terms]\n",
        "settlement_wiki = [t for terms in df['settlement_wiki'] for t in terms]\n",
        "nature_wiki = [t for terms in df['nature_wiki'] for t in terms]\n",
        "\n",
        "print(\"=== SETTLEMENT LEXICON ===\")\n",
        "print(f\"Synopsis: {len(settlement_syn)} mentions | Wikipedia: {len(settlement_wiki)} mentions\")\n",
        "print(f\"Top terms (synopsis): {Counter(settlement_syn).most_common(5)}\")\n",
        "\n",
        "print(\"\\n=== NATURE LEXICON ===\")\n",
        "print(f\"Synopsis: {len(nature_syn)} mentions | Wikipedia: {len(nature_wiki)} mentions\")\n",
        "print(f\"Top terms (synopsis): {Counter(nature_syn).most_common(5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Settlement/Nature frequency comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_synopsis = df['synopsis'].notna().sum()\n",
        "n_wiki = df['wikipedia_summary'].notna().sum()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Settlement comparison\n",
        "ax1 = axes[0]\n",
        "settlement_syn_counts = Counter(settlement_syn)\n",
        "settlement_wiki_counts = Counter(settlement_wiki)\n",
        "terms = settlement_lexicon\n",
        "x = range(len(terms))\n",
        "syn_vals = [settlement_syn_counts.get(t, 0) / n_synopsis * 100 for t in terms]\n",
        "wiki_vals = [settlement_wiki_counts.get(t, 0) / n_wiki * 100 for t in terms]\n",
        "ax1.bar([i - 0.2 for i in x], syn_vals, 0.4, label='Synopsis', color='steelblue')\n",
        "ax1.bar([i + 0.2 for i in x], wiki_vals, 0.4, label='Wikipedia', color='coral')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(terms, rotation=45, ha='right')\n",
        "ax1.set_ylabel('Mentions per 100 entries')\n",
        "ax1.set_title('Settlement Lexicon Frequency')\n",
        "ax1.legend()\n",
        "\n",
        "# Nature comparison\n",
        "ax2 = axes[1]\n",
        "nature_syn_counts = Counter(nature_syn)\n",
        "nature_wiki_counts = Counter(nature_wiki)\n",
        "terms = nature_lexicon\n",
        "x = range(len(terms))\n",
        "syn_vals = [nature_syn_counts.get(t, 0) / n_synopsis * 100 for t in terms]\n",
        "wiki_vals = [nature_wiki_counts.get(t, 0) / n_wiki * 100 for t in terms]\n",
        "ax2.bar([i - 0.2 for i in x], syn_vals, 0.4, label='Synopsis', color='steelblue')\n",
        "ax2.bar([i + 0.2 for i in x], wiki_vals, 0.4, label='Wikipedia', color='coral')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(terms, rotation=45, ha='right')\n",
        "ax2.set_ylabel('Mentions per 100 entries')\n",
        "ax2.set_title('Nature Lexicon Frequency')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Mobility/Static Lexicon\n\nUses spaCy lemmatization to identify:\n- **Stanziale (static)**: verbs like *vivere, abitare, restare, rimanere*\n- **Movimento (mobile)**: verbs like *partire, arrivare, fuggire, viaggiare*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define mobility/static lexicons\n",
        "stanziale_verbs = ['ambientare', 'svolgere', 'vivere', 'abitare', 'risiedere',\n",
        "                   'stabilire', 'restare', 'rimanere', 'rifugiare']\n",
        "stanziale_nouns = ['citt\u00e0 natale', 'paese natale', 'casa natale']\n",
        "\n",
        "movimento_verbs = ['partire', 'arrivare', 'raggiungere', 'tornare', 'ritornare',\n",
        "                   'rientrare', 'lasciare', 'abbandonare', 'fuggire', 'scappare',\n",
        "                   'trasferire', 'spostare', 'andare', 'recare', 'dirigere',\n",
        "                   'viaggiare', 'attraversare', 'percorrere', 'sbarcare',\n",
        "                   'approdare', 'migrare', 'inseguire', 'seguire']\n",
        "movimento_nouns = ['viaggio', 'fuga', 'ritorno', 'trasferimento', 'rientro', 'transito']\n",
        "\n",
        "def count_semantic_field(text, verb_lemmas, noun_lemmas):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    doc = nlp(str(text))\n",
        "    count = sum(1 for token in doc if token.pos_ == 'VERB' and token.lemma_.lower() in verb_lemmas)\n",
        "    text_lower = text.lower()\n",
        "    for noun in noun_lemmas:\n",
        "        count += len(re.findall(r'\\b' + noun + r'\\b', text_lower))\n",
        "    return count\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "print(\"Processing synopsis...\")\n",
        "df['stanziale_synopsis'] = df['synopsis'].progress_apply(lambda x: count_semantic_field(x, stanziale_verbs, stanziale_nouns))\n",
        "df['movimento_synopsis'] = df['synopsis'].progress_apply(lambda x: count_semantic_field(x, movimento_verbs, movimento_nouns))\n",
        "\n",
        "print(\"Processing wikipedia_summary...\")\n",
        "df['stanziale_wiki'] = df['wikipedia_summary'].progress_apply(lambda x: count_semantic_field(x, stanziale_verbs, stanziale_nouns))\n",
        "df['movimento_wiki'] = df['wikipedia_summary'].progress_apply(lambda x: count_semantic_field(x, movimento_verbs, movimento_nouns))\n",
        "\n",
        "print(\"\\nExtraction complete.\")\n",
        "print(f\"Synopsis - Stanziale: {df['stanziale_synopsis'].sum()}, Movimento: {df['movimento_synopsis'].sum()}\")\n",
        "print(f\"Wiki - Stanziale: {df['stanziale_wiki'].sum()}, Movimento: {df['movimento_wiki'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Mobility/Static distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Index distribution\n",
        "for ax, source, col_stan, col_mov in [(axes[0], 'Synopsis', 'stanziale_synopsis', 'movimento_synopsis'),\n",
        "                                       (axes[1], 'Wikipedia', 'stanziale_wiki', 'movimento_wiki')]:\n",
        "    n = df[col_stan.replace('stanziale_', '').replace('_synopsis', 'synopsis').replace('_wiki', 'wikipedia_summary')].notna().sum() if 'synopsis' in col_stan else df['wikipedia_summary'].notna().sum()\n",
        "    n = df['synopsis'].notna().sum() if 'synopsis' in col_stan else df['wikipedia_summary'].notna().sum()\n",
        "    \n",
        "    for idx, (col, label, color) in enumerate([(col_stan, 'Stanziale', 'steelblue'), (col_mov, 'Movimento', 'coral')]):\n",
        "        counts = df[col].value_counts().sort_index()\n",
        "        pcts = counts / n * 100\n",
        "        ax.bar([i + idx*0.4 for i in pcts.index[:8]], pcts.values[:8], 0.4, label=label, color=color)\n",
        "    ax.set_xlabel('Index value')\n",
        "    ax.set_ylabel('% of entries')\n",
        "    ax.set_title(f'{source}: Index Distribution')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top films by mobility/static scores (normalized)\n",
        "def count_tokens(text):\n",
        "    return len(nlp(str(text))) if pd.notna(text) else 0\n",
        "\n",
        "if 'tokens_synopsis' not in df.columns:\n",
        "    tqdm.pandas()\n",
        "    print(\"Counting tokens...\")\n",
        "    df['tokens_synopsis'] = df['synopsis'].progress_apply(count_tokens)\n",
        "    df['tokens_wiki'] = df['wikipedia_summary'].progress_apply(count_tokens)\n",
        "\n",
        "# Normalized scores (per 100 tokens)\n",
        "df['stanziale_syn_norm'] = df['stanziale_synopsis'] / (df['tokens_synopsis'] + 1) * 100\n",
        "df['movimento_syn_norm'] = df['movimento_synopsis'] / (df['tokens_synopsis'] + 1) * 100\n",
        "\n",
        "cols_display = ['reconciled_title', 'year', 'director', 'tokens_synopsis', 'stanziale_syn_norm', 'movimento_syn_norm']\n",
        "\n",
        "print(\"=== TOP 5 MOVIMENTO (Synopsis, normalized) ===\")\n",
        "print(df[df['tokens_synopsis'] > 50].nlargest(5, 'movimento_syn_norm')[cols_display].to_string(index=False))\n",
        "\n",
        "print(\"\\n=== TOP 5 STANZIALE (Synopsis, normalized) ===\")\n",
        "print(df[df['tokens_synopsis'] > 50].nlargest(5, 'stanziale_syn_norm')[cols_display].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n## 7. Visualization & Analysis\n\nComparative analysis of location extraction from both sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Prepare data\n",
        "synopsis_locs = [loc for locs in df['locations_synopsis'] for loc in (locs if isinstance(locs, list) else [])]\n",
        "wiki_locs = [loc for locs in df['locations_wiki'] for loc in (locs if isinstance(locs, list) else [])]\n",
        "\n",
        "synopsis_counts = Counter(synopsis_locs)\n",
        "wiki_counts = Counter(wiki_locs)\n",
        "\n",
        "print(f\"Total synopsis mentions: {len(synopsis_locs)}, unique: {len(set(synopsis_locs))}\")\n",
        "print(f\"Total wiki mentions: {len(wiki_locs)}, unique: {len(set(wiki_locs))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Location Overlap Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synopsis_set = set(synopsis_locs)\n",
        "wiki_set = set(wiki_locs)\n",
        "\n",
        "only_synopsis = synopsis_set - wiki_set\n",
        "only_wiki = wiki_set - synopsis_set\n",
        "both = synopsis_set & wiki_set\n",
        "\n",
        "print(f\"Only in synopsis: {len(only_synopsis)}\")\n",
        "print(f\"Only in wiki: {len(only_wiki)}\")\n",
        "print(f\"In both: {len(both)}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.pie([len(only_synopsis), len(only_wiki), len(both)],\n",
        "       labels=[f'Synopsis only\\n({len(only_synopsis)})', f'Wiki only\\n({len(only_wiki)})', f'Both\\n({len(both)})'],\n",
        "       colors=['steelblue', 'darkorange', 'purple'], autopct='%1.1f%%')\n",
        "ax.set_title('Location Overlap between Sources')\n",
        "plt.savefig('location_overlap.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top locations comparison\n",
        "total_synopsis = len(synopsis_locs)\n",
        "total_wiki = len(wiki_locs)\n",
        "\n",
        "all_top = set([x[0] for x in synopsis_counts.most_common(20)] + [x[0] for x in wiki_counts.most_common(20)])\n",
        "\n",
        "compare_df = pd.DataFrame({\n",
        "    'location': list(all_top),\n",
        "    'synopsis': [synopsis_counts.get(loc, 0) / total_synopsis * 100 for loc in all_top],\n",
        "    'wiki': [wiki_counts.get(loc, 0) / total_wiki * 100 for loc in all_top]\n",
        "}).sort_values('synopsis', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "y = range(len(compare_df))\n",
        "ax.barh([i - 0.2 for i in y], compare_df['synopsis'], 0.4, label='Synopsis', color='steelblue')\n",
        "ax.barh([i + 0.2 for i in y], compare_df['wiki'], 0.4, label='Wikipedia', color='darkorange')\n",
        "ax.set_yticks(y)\n",
        "ax.set_yticklabels(compare_df['location'])\n",
        "ax.set_xlabel('Percentage of mentions')\n",
        "ax.set_title('Top Locations Comparison (% of total mentions)')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_comparison_pct.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Frequency correlation scatter\n",
        "common_locs = list(both)\n",
        "x = [synopsis_counts[loc] / total_synopsis * 100 for loc in common_locs]\n",
        "y = [wiki_counts[loc] / total_wiki * 100 for loc in common_locs]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.scatter(x, y, alpha=0.6)\n",
        "for i, loc in enumerate(common_locs):\n",
        "    if x[i] > 2 or y[i] > 2:\n",
        "        ax.annotate(loc, (x[i], y[i]), fontsize=8)\n",
        "ax.set_xlabel('Synopsis frequency (%)')\n",
        "ax.set_ylabel('Wiki frequency (%)')\n",
        "ax.set_title('Location Frequency Correlation')\n",
        "ax.plot([0, max(x) if x else 1], [0, max(x) if x else 1], 'k--', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('location_correlation.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Interactive Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_locations_map(df, source='both'):\n",
        "    coords_with_labels = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        if source in ['synopsis', 'both']:\n",
        "            locations = row['locations_synopsis'] if isinstance(row['locations_synopsis'], list) else []\n",
        "            coordinates = row['coordinates_synopsis'] if isinstance(row['coordinates_synopsis'], list) else []\n",
        "            for loc, coord in zip(locations, coordinates):\n",
        "                if isinstance(coord, list) and len(coord) == 2 and all(isinstance(c, (int, float)) for c in coord):\n",
        "                    coords_with_labels.append((coord[0], coord[1], loc, 'synopsis'))\n",
        "        \n",
        "        if source in ['wiki', 'both']:\n",
        "            locations = row['locations_wiki'] if isinstance(row['locations_wiki'], list) else []\n",
        "            coordinates = row['coordinates_wiki'] if isinstance(row['coordinates_wiki'], list) else []\n",
        "            for loc, coord in zip(locations, coordinates):\n",
        "                if isinstance(coord, list) and len(coord) == 2 and all(isinstance(c, (int, float)) for c in coord):\n",
        "                    coords_with_labels.append((coord[0], coord[1], loc, 'wiki'))\n",
        "    \n",
        "    print(f\"Plotting {len(coords_with_labels)} location instances\")\n",
        "    \n",
        "    m = folium.Map(location=[42.5, 12.5], zoom_start=6)\n",
        "    heat_data = [[lat, lon] for lat, lon, _, _ in coords_with_labels]\n",
        "    HeatMap(heat_data, radius=10, blur=15).add_to(m)\n",
        "    \n",
        "    return m\n",
        "\n",
        "map_obj = create_locations_map(df)\n",
        "map_obj.save('locations_map.html')\n",
        "print(\"Saved: locations_map.html\")\n",
        "map_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.4 Final Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create flattened location table\n",
        "location_data = []\n",
        "for _, row in df.iterrows():\n",
        "    title = row.get('original_title', row.get('title', 'Unknown'))\n",
        "    year = row.get('year')\n",
        "    \n",
        "    if isinstance(row['locations_synopsis'], list) and isinstance(row['coordinates_synopsis'], list):\n",
        "        for loc, coord in zip(row['locations_synopsis'], row['coordinates_synopsis']):\n",
        "            if coord:\n",
        "                location_data.append({'film_title': title, 'year': year, 'location': loc,\n",
        "                                      'latitude': coord[0], 'longitude': coord[1], 'source': 'synopsis'})\n",
        "    \n",
        "    if isinstance(row['locations_wiki'], list) and isinstance(row['coordinates_wiki'], list):\n",
        "        for loc, coord in zip(row['locations_wiki'], row['coordinates_wiki']):\n",
        "            if coord:\n",
        "                location_data.append({'film_title': title, 'year': year, 'location': loc,\n",
        "                                      'latitude': coord[0], 'longitude': coord[1], 'source': 'wikipedia'})\n",
        "\n",
        "df_locations = pd.DataFrame(location_data)\n",
        "print(f\"Flattened table: {len(df_locations)} entries\")\n",
        "print(f\"  - From synopsis: {len(df_locations[df_locations['source']=='synopsis'])}\")\n",
        "print(f\"  - From wikipedia: {len(df_locations[df_locations['source']=='wikipedia'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grouped location summary\n",
        "locations_summary = df_locations.groupby(['location', 'latitude', 'longitude']).size().reset_index(name='count')\n",
        "locations_summary = locations_summary.sort_values('count', ascending=False)\n",
        "print(f\"Unique location-coordinate pairs: {len(locations_summary)}\")\n",
        "print(locations_summary.head(20).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final saves\n",
        "df.to_csv('database.csv', index=False)\n",
        "df_locations.to_csv('locations_flat.csv', index=False)\n",
        "locations_summary.to_csv('locations_coordinates.csv', index=False)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL OUTPUTS SAVED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"database.csv          - {len(df)} films with all data\")\n",
        "print(f\"locations_flat.csv    - {len(df_locations)} film-location pairs\")\n",
        "print(f\"locations_coordinates.csv - {len(locations_summary)} unique locations with counts\")\n",
        "print(f\"locations_map.html    - Interactive map\")\n",
        "print(f\"geocache.txt          - Coordinate cache for reuse\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}